# https://github.com/google-research/google-research/tree/master/goemotions/data/full_dataset
# Buradkai veri setiyle metin iÅŸleme yaparak. Gelen yorumdan o yorumdaki genel duygu tutumunu tahmin eden modeli geliÅŸtirelim.
# 1. fark => SÄ±nÄ±flandÄ±rma => 27 farklÄ± duygu tÃ¼rÃ¼
import datasets
import text_processor

#split => Veriyi bÃ¶l
data = datasets.load_dataset("go_emotions", split="train[:5000]")

texts = data['text']
labels = data['labels']

clean_texts = [text_processor.process_text(text) for text in texts]
augmented_texts = [text_processor.augment_text(text) for text in clean_texts]

print(texts[0])
print(clean_texts[0])
print(augmented_texts[0])

final_texts = clean_texts + augmented_texts
final_labels = list(labels) + list(labels)

print(len(final_texts))
print(len(final_labels))

print(final_texts[0])
print(final_labels[:50])

# EÄŸer labelimiz "sad,happy,love" gibi bir text label ise. [sad,happy,love,angry] => Encoding

# sad => 0
# happy => 1

# EÄŸer birden fazlaa label varsa bir veri iÃ§in. "sad","angry"
# MultiLabelBinarizer
# OneHotEncoder
# [6,9,27] => 10
import pickle 
from sklearn.preprocessing import MultiLabelBinarizer

mlb = MultiLabelBinarizer()
y = mlb.fit_transform(final_labels)
num_classes = y.shape[1]

with open("mlb.pkl", "wb") as f:
    pickle.dump(mlb, f)

# Label kÄ±smÄ±nÄ± hallettim. Derin Ã¶ÄŸrenmeye girmeye hazÄ±r.
# Text kÄ±smÄ±nÄ± halletmeliyim.

# Tokenization
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
tokenizer = Tokenizer(num_words=10000, oov_token="<OOV>")
tokenizer.fit_on_texts(final_texts)

with open("tokenizer.pkl", "wb") as f:
    pickle.dump(tokenizer, f)

sequences = tokenizer.texts_to_sequences(final_texts)
X = pad_sequences(sequences, padding="post", maxlen=100)

print(X[:50])
#

from tensorflow.keras.models import Sequential
from tensorflow.keras import layers
print("Durdur")
model = Sequential(
    [
        layers.Embedding(input_dim=10000, output_dim=128, input_length=100),
        layers.SpatialDropout1D(0.5), # Dropout => Embedding Ã§Ä±kÄ±ÅŸlarÄ±nda kullanÄ±lÄ±r. (Bir cÃ¼mleyi tamamen kullanmadan dropout yapar.)
        # Gated Recurrent Unit
        # Update Gate -> Ã–nceki bilgiyi ne kadar tutacaÄŸÄ±mÄ± belirliyor.
        # Reset Gate -> Ã–nceki bilgiyi ne kadar silineceÄŸini belirliyor.
        # LSTM'e gÃ¶re Daha az parametre ve daha hÄ±zlÄ±.
        # Forget Gate, -> GeÃ§miÅŸten gelen hangi bilgileri unutacaÄŸÄ±nÄ± belirler
        # Input Gate, -> Yeni gelen bilgiden ne kadarÄ±nÄ± belleÄŸe ekleyeceÄŸini belirler.
        # Output Gate -> Bellekteki bilginin ne kadarÄ±nÄ±n Ã§Ä±ktÄ±ya yansÄ±yacaÄŸÄ±nÄ± belirler.
        layers.Bidirectional(layers.GRU(128, return_sequences=True)),
        layers.LayerNormalization(),  # Katman Ã§Ä±ktÄ±larÄ±nÄ± normalize eder. (Ã–ÄŸrenmeyi daha stabil hale getirir.)
        layers.Bidirectional(layers.GRU(64, return_sequences=False)),
        # Ã–ÄŸrenilmiÅŸ bilgileri alÄ±p 64 farklÄ± karar mekanizmasÄ±ndan geÃ§ir.
        layers.Dense(64, activation="relu"),
        # Dense katmanÄ±ndaki karar mekanizmalarÄ±nÄ±n %50'sini rastgele kapat. Ezberlemeyi Ã¶nle.
        layers.Dropout(0.5),
        # Output Layer
        layers.Dense(num_classes, activation="sigmoid") # ToplamÄ± 1 olacak ÅŸekilde her bir duyguya oran verir. 
        # 0.1 => sad
        # 0.005 => happy
        # 0.85 => angry
        # 0.045 => love
    ]
)
import tensorflow as tf
# binary_crossentropy => Ã‡oklu etiket iÃ§in uygun bir loss function.
# Area Under The Curve => ROC eÄŸrisi altÄ±ndaki alanÄ± hesaplar.
# AUC => 0-1 arasÄ±nda deÄŸer alÄ±r 1 => mÃ¼kemmel 0.5 ve altÄ± ise kÃ¶tÃ¼.
model.compile(loss="binary_crossentropy", optimizer="adam", metrics=["accuracy", tf.keras.metrics.AUC(name="auc")])

model.summary()

model.fit(X,y, epochs=5,  batch_size=64, validation_split=0.2)

model.save("sentiment_analysis_model.h5")



# Data Augmentation (EDA) yaparak tÃ¼m veriyle bu modeli geliÅŸtirip skorlarÄ±nÄ± izlemek, 
# kendi Ã¼rettiÄŸimiz 10 veriyle test etmek.


from tensorflow.keras.models import load_model
import numpy as np
import pickle

# EÄŸitilmiÅŸ modeli ve yardÄ±mcÄ± dosyalarÄ± yÃ¼kle
model = load_model("sentiment_analysis_model.h5")

with open("tokenizer.pkl", "rb") as f:
    tokenizer = pickle.load(f)

with open("mlb.pkl", "rb") as f:
    mlb = pickle.load(f)

# Kendi yazdÄ±ÄŸÄ±mÄ±z 10 test verisi
test_texts = [
    "I feel really happy today!",
    "This makes me so angry.",
    "I donâ€™t know what to feel anymore...",
    "You are so cute and lovely ðŸ’•",
    "Honestly, I hate this so much.",
    "Nothing matters anymore.",
    "That was an amazing experience.",
    "Iâ€™m tired of everything.",
    "I feel motivated and ready to win!",
    "You broke my heart."
]

# Ã–n iÅŸleme (senin `text_processor.process_text` fonksiyonunu kullanmak istersen oraya uyarlayabilirsin)
def clean_text(text):
    import re
    text = text.lower()
    text = re.sub(r"http\S+", "", text)
    text = re.sub(r"[^a-zA-Z0-9\s]", "", text)
    text = re.sub(r"\s+", " ", text).strip()
    return text

clean_test_texts = [clean_text(t) for t in test_texts]
sequences = tokenizer.texts_to_sequences(clean_test_texts)
padded = pad_sequences(sequences, padding="post", maxlen=100)

# Tahmin et
predictions = model.predict(padded)

# Tahmin sonuÃ§larÄ±nÄ± gÃ¶ster
for i, probs in enumerate(predictions):
    top_indices = probs.argsort()[-3:][::-1]  # En yÃ¼ksek 3 tahmini al
    emotions = mlb.classes_[top_indices]
    scores = probs[top_indices]
    
    print(f"\nInput: {test_texts[i]}")
    for emo, score in zip(emotions, scores):
        print(f"  {emo}: {score:.2f}")
